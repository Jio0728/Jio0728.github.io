---
title: Variational Autoencoder
use_math: true
comments: true
published: true
---

## contents:
1. Variational AutoEncoder의 의미
2. 논문 설명

<br>
<br>
<br>
# 1. Variational Autoencoder의 의미

Variational Autoencoder 논문을 살펴보면, 타 딥러닝 논문에 비해 복잡하게 통계 위주로 내용이 전개됩니다. 이를 바로 이해하고, 직관적으로 받아들이는 것은 어려우므로 이번 장에서는 VAE의 의미에 대해서 살펴보고 다음 장에서 자세한 수식과 함께 설명하도록 하겠습니다.
이 장은 아래 블로그를 한 번 더 정리하여 작성하였습니다. 아래 블로그에 더욱 자세히 작성되어 있으니 의미를 이해하고 싶으신 분들은 참고하시길 바랍니다.
[Variational Autoencoder의 의미](https://gaussian37.github.io/dl-concept-vae/)
<br>
<br>
VAE의 처음 시작은 **2D random noise에서 원하는 사진을 얻을 수 있느냐?** 라는 질문에서 시작합니다.
<br>
<img width="712" alt="스크린샷 2022-09-19 오후 9 53 51" src="https://user-images.githubusercontent.com/87808237/191021911-30cfeb40-995e-4045-81ed-8943254d8bca.png">
<br>
한 픽셀이 0부터 256까지의 값을 가질 수 있다고 했을 때, 위 random noise가 가질 수 있는 이미지의 경우의 수는 $256^{Height\ast Width \ast 3(RGB)}$와 같습니다. 즉, 랜덤 노이즈를 그렸을 때, 그것이 내가 원하는 특정 이미지일 확률은 $\frac{1}{256^{Height\ast Width \ast 3(RGB)}}$입니다. 거의 불가능에 가깝습니다.   
그래서 VAE는 특정 이미지를 생성하기 쉬운 어떤 **분포**를 만드는 것을 목표합니다.
<br>
<br>
예를 들어, 우리가 비행기 사진을 어떤 분포로 표현한다고 가정합시다. 
<br>
<img width="696" alt="스크린샷 2022-09-19 오후 10 08 54" src="https://user-images.githubusercontent.com/87808237/191024795-77fb3814-c04f-4cfd-8bef-c9387c9addd8.png">
<br>
위 사진을 분포로 표현한다면, 다음과 같을 것입니다.   
위 사진에서 빨간 동그라미가 위치한 곳은 하늘이 그려져있습니다. 즉, 해당 부분 픽셀값의 분포는 파란색 주위로 밀집되어 있을 것입니다. 반면에, 노란 동그라미가 위치한 곳은 비행기가 그려져 있습니다. 때문에, 해당 부분의 픽셀값의 분포는 회색 주위로 밀집되어 있을 것입니다.    
이와 비슷하게 하늘 중앙에 비행기가 있는 사진들로 이루어진 데이터셋을 VAE 모델이 학습한다면 VAE는 사진의 가장자리(하늘)에 위치한 픽셀들의 값은 파란색 주위로 밀집되어 있고, 사진의 중앙(비행기)에 위치한 픽셀들의 값은 회색 주위로 밀집되어 있다는 그러한 **분포**를 학습하게 될 것입니다.   
그리고, **해당 분포에서 랜덤 노이즈를 뽑아낸다면** 우리는 훨씬 높은 확률로 우리가 원하는 이미지를 생성할 수 있게 됩니다.   
<br>
<br>
위 예시에서 설명한 사진의 특성을 우리는 latent variable이라고 부릅니다. 그리고 VAE는 latent variable의 학습을 목표합니다.
<br>
<img width="845" alt="스크린샷 2022-09-19 오후 10 14 08" src="https://user-images.githubusercontent.com/87808237/191025588-e577c4d4-17f3-403a-9506-9c501b943054.png">
<br>
<br>
그리고 해당 분포에서 랜덤 노이즈를 뽑아내어 새로운 이미지를 생성할 수 있게 됩니다.
<br>
<img width="873" alt="스크린샷 2022-09-19 오후 10 14 45" src="https://user-images.githubusercontent.com/87808237/191025730-9446b888-341e-4024-9adb-f6967c4f6531.png">
<br>
<br>
VAE는 Encoder 부분에서 input이미지를 토대로 latent variable을 생성하도록 학습합니다. 여기서 중요한 특성이 하나 있습니다. 바로, VAE는 latent variable을 표현해주는 **$\mu$와 $\sigma$를 생성**한다는 것입니다. 이는 gradient descent를 가능케 만들기 위함입니다. 자세한 설명은 뒤에서 하도록 하겠습니다.
<br>
<img width="853" alt="스크린샷 2022-09-19 오후 10 16 13" src="https://user-images.githubusercontent.com/87808237/191026013-08c2dcd7-e830-45dd-b91e-ae696ca42362.png">



<br>
# 2. 논문 설명
## 2-1. Background Knowledge
일단, 논문을 시작하기에 앞서 말씀드릴 게 있습니다. **통계적으로 아주 어렵습니다. Advanced statistical knowledge가 많이 나옵니다.**   
때문에, 일단 논문의 목적 이해하는데 필수적인 통계지식 세 가지만 짚고 넘어가겠습니다.
<br>
<br>
### MLE(Maximum Likelihood Estimator)
MLE는 가장 높은 가능도를 갖게 해주는 파라미터에 대한 estimator입니다. 즉, 현재 우리가 갖고 있는 데이터가 발생할 확률을 가장 높여주는 파라미터에 대한 estimator입니다.
<br>
<img width="692" alt="스크린샷 2022-09-19 오후 10 44 54" src="https://user-images.githubusercontent.com/87808237/191031864-722be7d7-dc00-4bac-a924-7cd8684ca1d7.png">
<br>
위 사진에서 파란 점들 각각이 datapoint를 나타내고, f1,f2,f3,f4는 서로 다른 파라미터들로 표현된 서로 다른 possibility density function(확률 분포)이라고 가정하겠습니다. datapoints들은 9와 10 사이로 밀집되어 있습니다.    
잠시, 현실세계에서 각 datapoint가 f4 분포를 따른다고 가정합시다. 위 그래프처럼 f4의 평균인 8이 아니라, 9와 10 사이에 datapoints가 밀집되어 있는 것은 다소 낮은 확률로 일어날 것입니다.    
반면에, 만약 각 datapoints가 f1을 따른다면, 위 그림과 같이 데이터가 분포되어 있는 것은 꽤 타당하고 현실성 있게 느껴질 것입니다.   
즉, ML의 관점에서는 위와 같이 datapoints가 분포되어 있을 때 f1의 분포가 현실성 있다고 판단할 것입니다. 그리고 더 나아가 f1의 분포를 나타내주는 parameter를 true parameter라고 추정할 것입니다.

### MAP (Maximum A Posteiroi)
MAP는 사후확률을 가장 높여주는 파라미터를 찾는 method입니다.
<br>
<img width="795" alt="스크린샷 2022-09-19 오후 10 51 59" src="https://user-images.githubusercontent.com/87808237/191033439-d56485f7-33e0-4d98-b4f5-52ea07d363ac.png">
<br>
위 사진은 변수 x를 기준으로 명명되었습니다.    
VAE의 관점에서 MAE를 작성하자면 다음과 같습니다.
$$ P_{\theta}\left ( z|x \right ) = \frac{P_{\theta}\left ( x|z \right ) \ast P_{\theta}\left ( z \right )}{P_{\theta}\left ( x \right )} $$

### Monte Carlo Estimation
해당 estimation은 반복된 무작위 추출로 함수값을 근사하는 것입니다.   
예시는 다음과 같습니다.    
예를 들어 단위 정사각형에 새겨진 사분원(원형 부분)을 생각해 보자. 몬테카를로 방법을 사용해서 {\displaystyle \pi }\pi의 값을 근사치로 추정할 수 있다.[13]

1.    정사각형을 그린 다음, 그 안에 사분원을 삽입한다.

2.    정사각형 위에 일정한 개수의 점을 균일하게 분포한다.

3.    사분원 내부의 점(즉, 원점으로부터 1 미만)의 개수를 센다.

4.    내부의 개수와 전체 개수의 비율은 두 영역의 비율을 나타낸다. 그 값에 4를 곱하여 $\pi$를 만든다.
<br>
<img width="240" alt="스크린샷 2022-09-19 오후 10 58 36" src="https://user-images.githubusercontent.com/87808237/191034832-58f0b312-6358-49e3-bc93-c78093ad96b6.png">
<br>
위 예시는 위키피디아에서 가져왔습니다. [위키피디아 Monte Carlo Estimation](https://ko.wikipedia.org/wiki/%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C_%EB%B0%A9%EB%B2%95)
<br>
<br>

## 2-2. VAE의 목적
VAE 논문의 목적은 다음과 같습니다.
'''
continuous latent variables의 interactable한 사후분포를 추정하겠다.
'''
위 목적을 보고 이해하는 것은 어렵습니다. 우리가 원래 아는 VAE의 목적을 참고해서 위 목적에 대해 설명하겠습니다.
<br>
<br>
먼저, VAE 모델의 목적을 다시 한 번 짚고 넘어가겠습니다. 
'''
모델을 통해서 생성된 이미지가 학습 이미지와 유사해야한다.
'''
<br>
위 목적은 **latent variables의 조합 별 이미지와 input 이미지의 유사도를 높여야한다.** 와 동일한 말입니다. 그리고 이것은 결국, VAE는 생성 이미지와 input 이미지 간의 유사도를 최대로 높여줄 $\mu$와 $\sigma$를 찾는다는 것과 동일한 말입니다.   
<br>
이미지 간 유사도를 **최대**로 높인다는 걸 보니, MLE가 떠오릅니다. 하지만, 위 문제에 MLE를 적용하는 것은 어렵습니다. MLE는 결국 특정 확률 분포를 알고 있을 때, 해당 분포에서 파라미터 값만 바꿔 maximum likelihood를 만들어내는 방법입니다. 이 방법을 위 목적에 적용하려면 latent variables의 조합 별 이미지와 input 이미지 사이 유사도의 분포를 알아야합니다. 이는 매우 복잡해서 현실적으로 어렵습니다.   
때문에 그 다음으로 적용할 방법이 MAP입니다. 하지만 MAP를 사용하려 해도 문제가 나타납니다. 
$$p_{\theta}\left (x \right) = \int p_{\theta}\left (z \right) p_{\theta}\left (x|z \right) dz$$
사전확률인 $p_{\theta}\left (x \right)$를 계산해야 하는데, 알 수 없는 latent variables z에 대해서 적분하는 것이 불가능합니다. (1장의 목적에서는 이해를 위해 latent variables들을 가시적으로 표현했지만, 실제로 우리는 각각의 latent variables들이 어떤 특징을 나타내는지 알 수 없습니다.)   
<br>
이 논문에서는 이렇게 사전확률조차 구할 수 없는 상황을 **Intractability**라고 명명하고, Intractable한 사후확률을 추론하는 것을 목표합니다.
<br>
<br>
<br>
## 2-2. Variational Inference - VAE의 핵심 포인트
그렇다면, MLE로도, MAP로도 추론할 수 있는 intractable posterior를 VAE는 어떻게 추론했을까요?   
<br>
키포인트는 **Variational Inference(VI)** 입니다.    
Variational Inference는 다루기 어려운 사후확률 $p\left (z|x \right)$를 우리가 알고 있는, 또는 다루기 쉬운 $q\left(z\right)$로 근사하는 것을 의미합니다. 











<br>
* * *
<br>
출처:
https://gaussian37.github.io/dl-concept-vae/
https://velog.io/@changdaeoh/vaereview
https://medium.com/humanscape-tech/paper-review-vae-ac918509a9ba
