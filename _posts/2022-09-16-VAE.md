---
title: Variational Autoencoder
use_math: true
comments: true
published: true
---

## contents:
1. Variational AutoEncoder의 의미
2. 논문 설명

<br>
<br>
<br>
# 1. Variational Autoencoder의 의미

Variational Autoencoder 논문을 살펴보면, 타 딥러닝 논문에 비해 복잡하게 통계 위주로 내용이 전개됩니다. 이를 바로 이해하고, 직관적으로 받아들이는 것은 어려우므로 이번 장에서는 VAE의 의미에 대해서 살펴보고 다음 장에서 자세한 수식과 함께 설명하도록 하겠습니다.
이 장은 아래 블로그를 한 번 더 정리하여 작성하였습니다. 아래 블로그에 더욱 자세히 작성되어 있으니 의미를 이해하고 싶으신 분들은 참고하시길 바랍니다.
[Variational Autoencoder의 의미](https://gaussian37.github.io/dl-concept-vae/)
<br>
<br>
VAE의 처음 시작은 **2D random noise에서 원하는 사진을 얻을 수 있느냐?** 라는 질문에서 시작합니다.
<br>
<img width="712" alt="스크린샷 2022-09-19 오후 9 53 51" src="https://user-images.githubusercontent.com/87808237/191021911-30cfeb40-995e-4045-81ed-8943254d8bca.png">
<br>
한 픽셀이 0부터 256까지의 값을 가질 수 있다고 했을 때, 위 random noise가 가질 수 있는 이미지의 경우의 수는 $256^{Height\ast Width \ast 3(RGB)}$와 같습니다. 즉, 랜덤 노이즈를 그렸을 때, 그것이 내가 원하는 특정 이미지일 확률은 $\frac{1}{256^{Height\ast Width \ast 3(RGB)}}$입니다. 거의 불가능에 가깝습니다.   
그래서 VAE는 특정 이미지를 생성하기 쉬운 어떤 **분포**를 만드는 것을 목표합니다.
<br>
<br>
예를 들어, 우리가 비행기 사진을 어떤 분포로 표현한다고 가정합시다. 
<br>
<img width="696" alt="스크린샷 2022-09-19 오후 10 08 54" src="https://user-images.githubusercontent.com/87808237/191024795-77fb3814-c04f-4cfd-8bef-c9387c9addd8.png">
<br>
위 사진을 분포로 표현한다면, 다음과 같을 것입니다.   
위 사진에서 빨간 동그라미가 위치한 곳은 하늘이 그려져있습니다. 즉, 해당 부분 픽셀값의 분포는 파란색 주위로 밀집되어 있을 것입니다. 반면에, 노란 동그라미가 위치한 곳은 비행기가 그려져 있습니다. 때문에, 해당 부분의 픽셀값의 분포는 회색 주위로 밀집되어 있을 것입니다.    
이와 비슷하게 하늘 중앙에 비행기가 있는 사진들로 이루어진 데이터셋을 VAE 모델이 학습한다면 VAE는 사진의 가장자리(하늘)에 위치한 픽셀들의 값은 파란색 주위로 밀집되어 있고, 사진의 중앙(비행기)에 위치한 픽셀들의 값은 회색 주위로 밀집되어 있다는 그러한 **분포**를 학습하게 될 것입니다.   
그리고, **해당 분포에서 랜덤 노이즈를 뽑아낸다면** 우리는 훨씬 높은 확률로 우리가 원하는 이미지를 생성할 수 있게 됩니다.   
<br>
<br>
위 예시에서 설명한 사진의 특성을 우리는 latent variable이라고 부릅니다. 그리고 VAE는 latent variable의 학습을 목표합니다.
<br>
<img width="845" alt="스크린샷 2022-09-19 오후 10 14 08" src="https://user-images.githubusercontent.com/87808237/191025588-e577c4d4-17f3-403a-9506-9c501b943054.png">
<br>
<br>
그리고 해당 분포에서 랜덤 노이즈를 뽑아내어 새로운 이미지를 생성할 수 있게 됩니다.
<br>
<img width="873" alt="스크린샷 2022-09-19 오후 10 14 45" src="https://user-images.githubusercontent.com/87808237/191025730-9446b888-341e-4024-9adb-f6967c4f6531.png">
<br>
<br>
VAE는 Encoder 부분에서 input이미지를 토대로 latent variable을 생성하도록 학습합니다. 여기서 중요한 특성이 하나 있습니다. 바로, VAE는 latent variable을 표현해주는 **$\mu$와 $\sigma$를 생성**한다는 것입니다. 이는 gradient descent를 가능케 만들기 위함입니다. 자세한 설명은 뒤에서 하도록 하겠습니다.
<br>
<img width="853" alt="스크린샷 2022-09-19 오후 10 16 13" src="https://user-images.githubusercontent.com/87808237/191026013-08c2dcd7-e830-45dd-b91e-ae696ca42362.png">



<br>
# 2. 논문 설명
## 2-1. Background Knowledge
일단, 논문을 시작하기에 앞서 말씀드릴 게 있습니다. **통계적으로 아주 어렵습니다. Advanced statistical knowledge가 많이 나옵니다.**   
때문에, 일단 논문의 목적 이해하는데 필수적인 통계지식 세 가지만 짚고 넘어가겠습니다.
<br>
<br>
### MLE(Maximum Likelihood Estimator)
MLE는 가장 높은 가능도를 갖게 해주는 파라미터에 대한 estimator입니다. 즉, 현재 우리가 갖고 있는 데이터가 발생할 확률을 가장 높여주는 파라미터에 대한 estimator입니다.
<br>
<img width="692" alt="스크린샷 2022-09-19 오후 10 44 54" src="https://user-images.githubusercontent.com/87808237/191031864-722be7d7-dc00-4bac-a924-7cd8684ca1d7.png">
<br>
위 사진에서 파란 점들 각각이 datapoint를 나타내고, f1,f2,f3,f4는 서로 다른 파라미터들로 표현된 서로 다른 possibility density function(확률 분포)이라고 가정하겠습니다. datapoints들은 9와 10 사이로 밀집되어 있습니다.    
잠시, 현실세계에서 각 datapoint가 f4 분포를 따른다고 가정합시다. 위 그래프처럼 f4의 평균인 8이 아니라, 9와 10 사이에 datapoints가 밀집되어 있는 것은 다소 낮은 확률로 일어날 것입니다.    
반면에, 만약 각 datapoints가 f1을 따른다면, 위 그림과 같이 데이터가 분포되어 있는 것은 꽤 타당하고 현실성 있게 느껴질 것입니다.   
즉, ML의 관점에서는 위와 같이 datapoints가 분포되어 있을 때 f1의 분포가 현실성 있다고 판단할 것입니다. 그리고 더 나아가 f1의 분포를 나타내주는 parameter를 true parameter라고 추정할 것입니다.

### MAP (Maximum A Posteiroi)
MAP는 사후확률을 가장 높여주는 파라미터를 찾는 method입니다.
<br>
<img width="795" alt="스크린샷 2022-09-19 오후 10 51 59" src="https://user-images.githubusercontent.com/87808237/191033439-d56485f7-33e0-4d98-b4f5-52ea07d363ac.png">
<br>
위 사진은 변수 x를 기준으로 명명되었습니다.    
VAE의 관점에서 MAE를 작성하자면 다음과 같습니다.
$$ P_{\theta}\left ( z|x \right ) = \frac{P_{\theta}\left ( x|z \right ) \ast P_{\theta}\left ( z \right )}{P_{\theta}\left ( x \right )} $$

### Monte Carlo Estimation
해당 estimation은 반복된 무작위 추출로 함수값을 근사하는 것입니다.   
예시는 다음과 같습니다.    
예를 들어 단위 정사각형에 새겨진 사분원(원형 부분)을 생각해 보자. 몬테카를로 방법을 사용해서 $\pi$의 값을 근사치로 추정할 수 있습니다.

1.    정사각형을 그린 다음, 그 안에 사분원을 삽입한다.

2.    정사각형 위에 일정한 개수의 점을 균일하게 분포한다.

3.    사분원 내부의 점(즉, 원점으로부터 1 미만)의 개수를 센다.

4.    내부의 개수와 전체 개수의 비율은 두 영역의 비율을 나타낸다. 그 값에 4를 곱하여 $\pi$를 만든다.
<br>
<img width="240" alt="스크린샷 2022-09-19 오후 10 58 36" src="https://user-images.githubusercontent.com/87808237/191034832-58f0b312-6358-49e3-bc93-c78093ad96b6.png">
<br>
위 예시는 위키피디아에서 가져왔습니다. [위키피디아 Monte Carlo Estimation](https://ko.wikipedia.org/wiki/%EB%AA%AC%ED%85%8C%EC%B9%B4%EB%A5%BC%EB%A1%9C_%EB%B0%A9%EB%B2%95)

<br>
<br>

## 2-2. VAE의 목적
VAE 논문의 목적은 다음과 같습니다.
'''
모델을 통해서 생성된 이미지가 학습 이미지와 유사해야한다.
'''
<br>
위 목적은 **latent variables의 조합 별 이미지와 input 이미지의 유사도를 높여야한다.** 와 동일한 말입니다. 그리고 이 목적에는 두 가지 절차가 필요합니다.
  1. VAE는 생성 이미지와 input 이미지 간의 유사도를 최대로 높여줄 $\mu$와 $\sigma$를 찾아서 이미지를 잘 표현해주는 prior distribution $p_{\theta^{*}}\left(z\right)$를 찾는다.
  2. True posterior density $p_{theta}\left(z|x\right)$에서 latent variables(z)의 조합별 이미지(x)를 뽑아낸다.
<br>
1번째 절차에서 이미지 간 유사도를 **최대**로 높인다는 걸 보니, MLE가 떠오릅니다. 하지만, 위 문제에 MLE를 적용하는 것은 어렵습니다. MLE는 결국 특정 확률 분포를 알고 있을 때, 해당 분포에서 파라미터 값만 바꿔 maximum likelihood를 만들어내는 방법입니다. 이 방법을 위 목적에 적용하려면 latent variables의 조합 별 이미지와 input 이미지 사이 유사도의 분포를 알아야합니다. 이는 매우 복잡해서 현실적으로 어렵습니다.   
때문에 그 다음으로 적용할 방법이 MAP입니다. 하지만 MAP를 사용하려 해도 문제가 나타납니다. 
$$p_{\theta}\left (x \right) = \int p_{\theta}\left (z \right) p_{\theta}\left (x|z \right) dz$$
사전확률인 $p_{\theta}\left (x \right)$를 계산해야 하는데, 알 수 없는 latent variables z에 대해서 적분하는 것이 불가능합니다. (이 글의 1장 - 목적에서는 이해를 위해 latent variables들을 가시적으로 표현했지만, 실제로 우리는 각각의 latent variables들이 어떤 특징을 나타내는지 알 수 없습니다.)   
<br>
이 논문에서는 위 목적을 이루기 위해 필요한 분포들에 대해 아무런 가정을 하지 않습니다. 해당 분포들은 true posterior density인 $p_{\theta}\left (z|x \right) = \frac {p_{\theta}\left (x|z \right) p_{\theta}\left (z \right) dz}{p_{\theta}\left (x \right)}$, 그리고 MLE와 MAP와 같은 [mean-field](https://en.wikipedia.org/wiki/Mean-field_theory) 방법을 사용하는데 필요한 모든 적분을 포함합니다. 그리고 이런 분포들이 모두 intractable한 상황을 가정하고, 이런 상황에서도 general하게 작동할 수 있는 알고리즘(위 목적을 위한) 개발을 목표합니다.
<br>
<br>
<br>
## 2-2. Variational Inference - VAE의 핵심 포인트
VAE는 **Variational Inference(VI)** 에 기반하여 $p_{theta}$ 분포의 추정에 새로운 접근을 합니다.        
Variational Inference는 다루기 어려운 사후확률 $p\left (z|x \right)$를 우리가 알고 있는, 또는 다루기 쉬운 $q\left(z\right)$를 이용하여 근사하는 것을 의미합니다.   
이 때 VI는 Kullback–Leibler divergence를 활용하여 두 분포 간 유사도를 높이는 방향으로 $q\left(z\right)$를 업데이트 합니다. 즉, $D_{KL}\left(p\left (z|x \right)||q\left (z \right)\right)$를 줄이는 방향으로 $q\left(z\right)$를 업데이트 합니다.     
이는 꽤 중요한 포인트인데, 지금까지 statistical inference의 문제였던 사후분포 추정 문제를 두 분포 간 유사도를 높이는 **optimization** 문제로 변경한 것이기 때문입니다.    
VI가 해당 논문에서 어떻게 사용되는지는 다음 장에서 자세히 설명하겠습니다.
<br>
<br>
<br>
## 2-3. Variational Bound
VI의 Kullback-Leibler divergence를 변형해보겠습니다. Kullback-Leiber divergence의 공식은 다음과 같습니다.    
$$D_{KL}\left(p\left (z|x \right)||q\left (z \right)\right)= \sum_{x\in X}P\left(x\right)log\left(\frac{P\left(x\right)}{Q\left(x\right)}\right)$$
위 식을 어떻게 저렇게 전개하면 다음과 같은 식이 나옵니다.
$$logp_{\theta}\left(x\right) = D_{KL}\left(p\left (z|x \right)||q\left (z \right)\right) +\mathbb{E}_{q_{\phi}\left(z|x\right)}\left [-logq_{\phi}\left(z|x\right) + logp_{\theta}\left(x,z\right)\right ]$$
위 식의 좌변인 $log_{\theta}\left(x\right)$는 우리에게 중요한 의미를 갖습니다. 해당 식은 우리에게 데이터셋 x가 주어졌을 때, 파라미터 $\theta$가 나올 확률을 의미합니다. 해당 확률을 최대화하면, 우리는 $\theta$에 대한 MLE를 얻게 됩니다. 즉, input 이미지와 유사한 이미지를 생성할 수 있게 해주는 파라미터 $\theta$를 얻게 된다는 것입니다.
<br>
위 식의 우변의 KL divergence는 항상 0 이상이기 때문에, $\mathbb{E}_{q_{\phi}\left(z|x\right)}\left [ -logq_{\phi}\left(z|x\right) + logp_{\theta}\left(x,z\right)\right ]$는 $logp_{\theta}\left(x\right)$의 lower bound의 역할을 하게 됩니다.  
<br>
위 식에서 구한 lower bound를 최대화 하면 intractable한 marginal likelihood $logP\left(x\right)$ 또한 최대화 할 수 있게 됩니다.
<br>
위 식의 lower bound를 또 어떻게 저렇게 변형하면 다음의 식이 나옵니다.
$$logp_{\theta}\left(x^{\left(i\right)}\right) \geq \mathbb{E}_{q_{\phi}\left(z|x\right)}\left [-logq_{\phi}\left(z|x\right) + logp_{\theta}\left(x,z\right)\right ] $$
$$= -D_{KL}\left(q_{\phi}\left (z|x^{\left(i\right)} \right)||p_{\theta}\left (z \right)\right) +\mathbb{E}_{q_{\phi}\left(z|x^{\left(i\right)}\right)}\left [logp_{\theta}\left(x^{\left(i\right)}|z\right)\right ]$$
<br>
이처럼 우리는 $p_{\theta}\left(x\right)$의 lower bound에 대하여 두 가지 식을 얻게 되었습니다. 
자세한 유도는 [이 글](https://medium.com/humanscape-tech/paper-review-vae-ac918509a9ba)을 참고해주세요.

<br>
<br>
## 2-4. Reparameterization
이제 우리의 목적은 저번 장에서 구한 lower bound를 최대화입니다. 딥러닝에서 보통 최대화는 gradient를 계산하여 진행합니다. 하지만 위 lower bound에는 sampling이 포함되어 있습니다. sampling을 미분하는 것은 불가능합니다. 이제 우리는 lower bound를 미분 가능한 형태로 바꿔줘야 합니다.     
<br>
VAE 논문에서는 reparameterization을 활용합니다.     
reparametrization이란, $q_{\phi}\left(z|x\right)$를 따르는 z를 $\widetilde{z} = g_{\phi}\left(\varepsilon , x\right)$ 로 변형해주는 것을 의미합니다. 이 때, $\varepsilon \sim p\left(\varepsilon \right)$입니다.      
이러한 deterministic function을 이용하여 변형한 $\widetilde{z}$를 monte carlo gradient estimator에 넣으면 다음과 같은 식이 탄생합니다.(왜 Monte Carlo Expectation 식을 활용하는지도 설명 드리고 싶지만 아직 베이지안을 공부하지 않았어서 설명이 부족합니다ㅠ 혹시 아시는 분 계시다면 댓글 달아주시면 감사하겠습니다!)  
<br>
<img width="928" alt="스크린샷 2022-09-22 오전 10 21 08" src="https://user-images.githubusercontent.com/87808237/191637649-ccbc76c6-1245-4e39-8429-b00bf8fbd3ac.png">
<br>
위 식의 $f\left(z\right)$는 어떠한 함수의 general form입니다.    
우리는 이전 2-3. Variational Bound에서 Lower Bound에 대한 두 가지 식을 배웠습니다.
$$\mathbb{L}\left(\theta, \phi ;x^{\left(i\right)}\right) = \mathbb{E}_{q_{\phi}\left(z|x\right)}\left [ -logq_{\phi}\left(z|x\right) + logp_{\theta}\left(x,z\right)\right ]$$
$$\mathbb{L}\left(\theta, \phi ;x^{\left(i\right)}\right) = -D_{KL}\left(q_{\phi}\left (z|x^{\left(i\right)} \right)||p_{\theta}\left (z \right)\right) +\mathbb{E}_{q_{\phi}\left(z|x^{\left(i\right)}\right)}\left [logp_{\theta}\left(x^{\left(i\right)}|z\right)\right ]$$
<br>
방금 구한 Monte Carlo Gradient Estimator를 두 lower bound 식에 적용하면 각각 다음과 같은 식으로 전개됩니다.  
<br>
<br>
첫번째 lower bound 식
<br>
<img width="636" alt="스크린샷 2022-09-22 오전 10 21 42" src="https://user-images.githubusercontent.com/87808237/191637712-eabbca50-ff61-411a-90fb-902cb692be57.png">
<br>
두번째 lower bound 식
<br>
<img width="715" alt="스크린샷 2022-09-22 오전 10 21 50" src="https://user-images.githubusercontent.com/87808237/191637716-4e74b362-fc82-4dbc-8c62-d1d3ddfd1abf.png">
<br>
<br>
위 lower bound식에서 L은 각 데이터 당 만들어지는 샘플의 개수를 의미합니다. (한 개의 인풋 이미지에 대해 L개의 샘플을 생성한다는 뜻입니다.)
<br>
<br>
또한 여러 개의 데이터로 이루어진 데이터셋이 주어졌을 때, 미니배치를 활용하여 lower bound를 표현할 수도 있습니다.     
총 N개의 데이터가 있고, 각 배치가 M개의 데이터로 이루어져있다고 가정했을 때, lower bound식은 다음과 같이 전개됩니다.
<br>
<img width="547" alt="스크린샷 2022-09-22 오전 10 21 59" src="https://user-images.githubusercontent.com/87808237/191637727-6c94bf8a-0d5a-445b-b84f-dddcc4d18827.png">
<br>
이 때, VAE 논문에서는 실험을 통해 미니배치의 크기인 M이 충분히 크다면 한 데이터포인트 당 샘플의 개수인 L이 1로 설정되어도 괜찮다는 것을 확인했다고 얘기합니다.     
이 미니배치 식을 통해서 Derivatives $\bigtriangledown_{\theta, \phi} \widetilde{\mathbb{L}}\left(\theta;X^{M}\right)$를 구할 수 있고, 해당 gradient는 SGD 또는 Adagrad와 함께 활용될 수 있습니다.
자세한 유도는 [이 글](https://medium.com/humanscape-tech/paper-review-vae-ac918509a9ba)을 참고해주세요.
<br>
<br>
VAE 논문에서는 두 번째 Lower Bound식과 auto-encoder와의 관계성도 얘기하고 있습니다.
<br>
<img width="715" alt="스크린샷 2022-09-22 오전 10 21 50" src="https://user-images.githubusercontent.com/87808237/191637716-4e74b362-fc82-4dbc-8c62-d1d3ddfd1abf.png">
<br>
위 식 우변의 첫번째 항은 regularizer로, 두번째 항은 expected negative reconstruction error라고 얘기합니다.
<br>
첫번째 항부터 보겠습니다.$ - D_{KL}\left(q_{\phi}\left (z|x^{\left(i\right)} \right)||p_{\theta}\left (z \right)\right)$는 $\widetilde{\mathbb{L}}^{B}\left(\theta;x^{\left(i\right)}\right)$를 최대화하기 위해서는 최소화가 되어야 합니다. 즉, 주어진 두 분포의 유사도를 높여주는 방향으로 regularization이 들어갈 것입니다.
<br>
<br>
두 번째 항 $\frac{1}{L}\sum_{l=1}^{L}\left(logp_{\theta}\left(x^{\left(i\right)}|z^{\left(i, l\right)}\right)\right)$은 $logp_{\theta}\left(x^{\left(i\right)}|z^{\left(i, l\right)}\right)$의 평균입니다. 이 때, $p_{\theta}\left(x^{\left(i\right)}|z^{\left(i, l\right)}\right)$는 $\theta$로 정의된 분포에서 z가 주어졌을 때 x가 나올 확률을 얘기합니다. 즉, input 이미지로부터 뽑아낸 latent vector로부터 해당 이미지를 재구성할 확률입니다. 확률은 (0,1)의 범위를 가지기 때문에 $logp_{\theta}\left(x^{\left(i\right)}|z^{\left(i, l\right)}\right)$는 $\theta$는 무조건 음수의 값을 가지게 됩니다. 그리고 확률이 낮아질 수록 더욱 작은 음수의 값을 가지기 때문에 negative reconstruction error라고 부릅니다.

* * *
논문의 reparameterization과 experiment부분은 생략하고, 여기에서 VAE 논문 리뷰를 마치도록 하겠습니다.     
논문을 읽으며 베이지안 지식의 필요성을 절감하였습니다.ㅠ.ㅠ









<br>
* * *
<br>
출처:
<br>
https://gaussian37.github.io/dl-concept-vae/
<br>
https://velog.io/@changdaeoh/vaereview
<br>
https://medium.com/humanscape-tech/paper-review-vae-ac918509a9ba
