---
title: Encoder와 ViT 기본 개념 정리
use_math: true
comment: true
published: false
---

### content
1. Encoder
  a. Encoder의 입력: Embedding & Positional Encoding
  b. Residual Connection
  c. Feed-forward layer
  d. Layer Normalization
2. ViT
3. ViT for face recognition
<br>
이번 게시글의 주제는 ViT이다. 때문에 Encoder에 대해서는 간략하게만 집고, ViT 내용으로 바로 넘어가도록 하겠다.

## 1. Encoder
Transformer는 Encoder와 Decoder로 이루어져있다. 이 중 Encoder는 다음의 구조를 가지고 있다.
![1_UKE4mej2zSci-CoRYKb3uQ](https://user-images.githubusercontent.com/87808237/188880192-b01a45b6-6262-4b65-b937-01fd227f7928.png)

### a. Encoder의 입력: Embedding & Positional Encoding
Encoder는 Embedding과 Positional Encoding의 합이 입력값으로 들어간다. 문장의 각 단어를 하나씩 처리하는 RNN과 달리 Transformer는 단어들을 병렬적으로 처리하기 때문에, input 문장의 단어의 위치 정보(순서)를 잃어버린다. 이를 보완하기 위해서, 각 단어의 순서를 나타내는 positional encoding이 들어간다.   
<br>
Positional Encoding은 다음과 같은 조건을 만족해야 한다.
1. 각 단어/time step애 대해서 유일한 encoding을 출력해야 한다.
2. encoding의 값의 상한선과 하한선이 정해져있어야한다.
3. 간격이 동일하다면, 위치와 관계 없이 encoding사이의 간격 또한 동일해야 한다. 또는, 문장의 길이가 늘어나더라도 encoding 값은 변경되면 안 된다. (consistency across sentences)
<br>
해당 조건을 만족하는 Encoding 방법 중 하나는 **Sinusoidal Position Encoding** 이다. 해당 Encoding 공식은 아래와 같다.
<img width="746" alt="스크린샷 2022-09-07 오후 9 54 35" src="https://user-images.githubusercontent.com/87808237/188883411-06867098-07c5-4eb1-8909-9f436cf22d3d.png">
Positional Encoding에 대한 더욱 자세한 사항은 아래 블로그를 참고하자. 아주 잘 정리되어 있다.
https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding

Input 행렬의 각 단어는 d차원의 벡터로 임베딩 된다. 각 단어는 Embedding matrix의 각 행을 이루게 되므로, Embedding matrix는 n\*d 차원이다. (n: 단어의 개수)   
Positional Embedding은 Embedding Matrix와 차원을 맞춰서 생성된다. 즉, 마찬가지로 n\*d차원이다. 이 때 Positional Encodding의 각 행은 Embedding Matrix에서 대응되는 행이 가리키는 단어의 Positional encoding vector이다. 이렇게 생성된 두 matrix는 summation 되어 Encoder의 입력으로 들어간다.

### b. Residual Connection
Encoder 내부에서는 컴퓨터 비전의 ResNet에서 쓰인 개념인 Residual Connection이 들어가있다. 즉, Multi-head Attention에 들어가는 입력값과, Multi-head Attention의 결과 출력값이 합해져서 다음 단계로 넘어가게 된다.

### c. Feed-forward layer
Feed-forward layer는 우리가 아는 fc layer와 동일한 것이다. ReLU 함수를 포함한 2개의 layer로 구성되어 있다.

### d. Layer Normalization
해당 층은 훈련을 안정적으로 하기 위해 들어가있다.
<br>
<br>
## 2. ViT
다음은 ViT에 대해 정리하겠다. 

### a. Inductive Bias
Inductive Bias란 모델이 데이터로 훈련을 하면서 얻게 되는, 사전에 보지 않은 데이터도 적절히 **추론** 할 수 있도록 만들어주는 어떠한 형태의 **편견** 이다.   
예를 들어, CNN이 쓰인 모델은 필터가 이미지의 여러 영역을 이동하며 convolution을 진행하기 때문에 위치에 대한 inductive bias를 갖게 된다. RNN을 쓰는 모델은 단어가 순차적으로 들어오기 때문에 어떠한 순차적 정보를 가정하게 된다. (데이터가 순차적일 것이라는 Inductive bias를 가지게 된다.)   
여러 영역을 이동하는 CNN, sequence가 순차적으로 들어오는 RNN과 달리 Transformer는 병렬적으로 데이터를 처리한다. 때문에 inductive bias가 적은 편이다. 때문에 소량의 데이터로 학습할 경우에는 낮은 inductive bias로 인해 다른 CNN, RNN 모델보다 낮은 정확도를 보인다.    
ViT에서는 오직 MLP 만이 locality와 translational equivariance라는 inductive bias를 가지고 있고 self-attention layer는 global 하다. 때문에 다음과 같은 방식으로 부족한 inductive bias를 보충한다.
1. 이미지를 패치로 자르기
2. 파인 튜닝 시 다른 해상도의 이미지들에게 positional encoding을 맞추기

### b.전체 구조
vit의 구조는 다음과 같다.
![0](https://user-images.githubusercontent.com/87808237/188885307-8f0ced1d-0c13-4c94-bb9d-0ae61ddc3b09.png)
Input 이미지의 크기가 H\*W\*C라고 하자. 이 때 H는 높이 픽셀 수, W는 너비 픽셀 수, C는 채널의 수다. 이 Input 이미지를 N개의 패치로 나눈다. 이 때, 각 패치의 크기는 P\*P\*C이다.   
H\*W\*C = N\*(P^2\*C)   
각각의 패치는 Linear Projecton of Falttend Patches를 통과하여 D차원의 벡터로 변환된다. 변환된 각 벡터는 위 그림에서 핑크색 동그라미로 표현되어 있다.   
이 D차원의 벡터에 class embedding(\*)이 합쳐지고, positional encoding이 더해져서 transformer encoder에 들어가게 된다.   
Encoder의 output은 2-layer로 이루어지고 GeLU가 포함된 MLP Head에 들어가게 되고, MLP Head의 결과 클래스가 나오게 된다.

### c. class token
vit에서 패치의 앞에 붙는 class token은 Bert의 class token과 비슷한 역할을 한다.   
Bert는 transformer 모델로, 한 단어를 embedding 할 때 다른 단어와의 연관성이 포함되어 embedding 된다. (즉, 한 단어의 embedding 값은 그 단어의 뜻 뿐만이 아니라 그 단어와 다른 단어 사이의 관계도 내포하고 있다.) 이 때 문장의 맨 앞에 붙는 class token은 그 자체로는 어떠한 뜻도 가지고 있지 않다. 때문에 class token의 임베딩 벡터는 다른 토큰과 주고받은 정보를 포함한다. 즉, 문장 전체의 뜻을 represent하는 기능을 갖고 있다.   
때문에 vit의 class token 또한 Encoder를 거치면 이미지를 대표하는 벡터가 된다. 해당 벡터가 MLP를 거치면 우리는 이미지가 대표하는 클래스를 알 수 있게 된다.

### d. Hybrid Architecture
raw 이미지가 아닌 CNN을 통과한 Feature Map을 Input으로 Input sequence를 구성할 수도 있다. 이 때 feature map의 patch들은 이미 공간적 정보를 포함하고 있기 때문에 패치 크기를 1\*1으로 설정하기도 한다. 1\*1 패치 크기를 사용하는 경우, 이것은 결국 이미지를 flatten하여 input sequence를 구성한다는 것을 의미한다.

### e. Fine-tuning and higher resolution



* * *
Reference:   
https://moon-walker.medium.com/transformer-%EB%B6%84%EC%84%9D-2-transformer%EC%9D%98-encoder-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-1edecc2ad5d4   
https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding   
https://www.youtube.com/watch?v=0kgDve_vC1o&t=1253s    
https://junklee.tistory.com/117
